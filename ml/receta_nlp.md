
# üß™ Receta para Modelos de NLP: Una Gu√≠a Pr√°ctica para Cient√≠ficos de Datos

## üìö Ingredientes B√°sicos

### Herramientas Fundamentales
- **Python 3.8+**: Nuestro lenguaje base
- **IDE**: VSCode o PyCharm con soporte para Jupyter
- **Control de versiones**: Git
- **Entorno virtual**: conda o venv

### Librer√≠as Esenciales
- **Procesamiento de datos**: pandas, numpy
- **NLP**: spaCy, NLTK, transformers (Hugging Face)
- **Machine Learning**: scikit-learn
- **Deep Learning**: PyTorch o TensorFlow
- **Visualizaci√≥n**: matplotlib, seaborn

### Recursos Computacionales
- M√≠nimo: 16GB RAM, CPU multi-core
- Recomendado: GPU NVIDIA con 8GB+ VRAM
- Alternativa: Google Colab Pro o AWS SageMaker

## üë©‚Äçüç≥ Preparaci√≥n (Pre-procesamiento)

### 1. Recolecci√≥n de Datos
**¬øQu√©?** Obtener un dataset limpio y representativo.

**¬øC√≥mo?**
1. Identifica fuentes confiables (Kaggle, UCI, datos propios)
2. Establece criterios de calidad
3. Define el formato de los datos

**Herramientas:**
```python
import pandas as pd
from datasets import load_dataset  # Hugging Face datasets

# Ejemplo de carga
dataset = load_dataset('csv', data_files='datos.csv')
# o
df = pd.read_csv('datos.csv')
```

**Documentaci√≥n:**
- [Hugging Face Datasets](https://huggingface.co/docs/datasets/)
- [Pandas Documentation](https://pandas.pydata.org/docs/)

### 2. Limpieza y Normalizaci√≥n
**¬øQu√©?** Preparar el texto para su procesamiento.

**¬øC√≥mo?**
1. Eliminar ruido (caracteres especiales, HTML)
2. Normalizar texto (min√∫sculas, acentos)
3. Tokenizaci√≥n
4. Eliminar stopwords (opcional)

**Herramientas:**
```python
import spacy
import re
from unidecode import unidecode

nlp = spacy.load('es_core_news_sm')

def limpiar_texto(texto):
    texto = re.sub(r'<[^>]+>', '', texto)  # Eliminar HTML
    texto = unidecode(texto.lower())  # Normalizar
    doc = nlp(texto)
    tokens = [token.text for token in doc if not token.is_stop]
    return ' '.join(tokens)
```

**Documentaci√≥n:**
- [spaCy Course](https://course.spacy.io/)
- [Regular Expressions](https://docs.python.org/3/library/re.html)

## ü•ò Preparaci√≥n del Modelo

### 3. Vectorizaci√≥n del Texto
**¬øQu√©?** Convertir texto en vectores num√©ricos.

**¬øC√≥mo?**
1. Elegir m√©todo de vectorizaci√≥n:
   - Bag of Words
   - TF-IDF
   - Word Embeddings
   - Embeddings contextuales

**Herramientas:**
```python
from sklearn.feature_extraction.text import TfidfVectorizer
from transformers import AutoTokenizer, AutoModel

# TF-IDF
vectorizer = TfidfVectorizer(max_features=10000)
X = vectorizer.fit_transform(textos)

# BERT Embeddings
tokenizer = AutoTokenizer.from_pretrained('dccuchile/bert-base-spanish-wwm-uncased')
model = AutoModel.from_pretrained('dccuchile/bert-base-spanish-wwm-uncased')
```

**Documentaci√≥n:**
- [Scikit-learn: Vectorizaci√≥n de texto](https://scikit-learn.org/stable/modules/feature_extraction.html#text-feature-extraction)
- [Transformers Documentation](https://huggingface.co/transformers/)

### 4. Divisi√≥n y Evaluaci√≥n
**¬øQu√©?** Preparar conjuntos de entrenamiento y prueba.

**¬øC√≥mo?**
1. Dividir datos (t√≠picamente 80/20)
2. Establecer m√©tricas de evaluaci√≥n
3. Implementar validaci√≥n cruzada

**Herramientas:**
```python
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.metrics import classification_report

X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42
)
```

## üç≥ Cocci√≥n (Entrenamiento)

### 5. Selecci√≥n y Entrenamiento del Modelo
**¬øQu√©?** Elegir y entrenar el modelo adecuado.

**¬øC√≥mo?**
1. Seleccionar arquitectura seg√∫n tarea:
   - Clasificaci√≥n: BERT, RoBERTa
   - Secuencias: LSTM, Transformer
   - Generaci√≥n: GPT, T5
2. Ajustar hiperpar√°metros
3. Implementar early stopping

**Herramientas:**
```python
from transformers import Trainer, TrainingArguments
from sklearn.linear_model import LogisticRegression

# Modelo simple
modelo = LogisticRegression()
modelo.fit(X_train, y_train)

# Modelo transformers
training_args = TrainingArguments(
    output_dir="./resultados",
    num_train_epochs=3,
    per_device_train_batch_size=16,
    evaluation_strategy="epoch"
)
```

### 6. Optimizaci√≥n y Ajuste
**¬øQu√©?** Mejorar el rendimiento del modelo.

**¬øC√≥mo?**
1. Analizar errores comunes
2. Ajustar hiperpar√°metros
3. Probar t√©cnicas de regularizaci√≥n
4. Implementar t√©cnicas de data augmentation

**Herramientas:**
```python
from sklearn.model_selection import GridSearchCV

param_grid = {
    'C': [0.1, 1, 10],
    'max_iter': [100, 200, 300]
}

grid_search = GridSearchCV(
    LogisticRegression(), 
    param_grid, 
    cv=5
)
grid_search.fit(X_train, y_train)
```

## üçΩÔ∏è Servido (Deployment)

### 7. Evaluaci√≥n Final y Documentaci√≥n
**¬øQu√©?** Validar y documentar el modelo.

**¬øC√≥mo?**
1. Evaluar en conjunto de prueba
2. Documentar decisiones y resultados
3. Crear gu√≠as de uso
4. Establecer l√≠neas base de rendimiento

**Herramientas:**
```python
import mlflow
import pickle

# Guardar modelo
with open('modelo.pkl', 'wb') as f:
    pickle.dump(modelo, f)

# Logging con MLflow
mlflow.log_param("vectorizer", "tfidf")
mlflow.log_metric("accuracy", accuracy)
```

### 8. Despliegue y Monitoreo
**¬øQu√©?** Poner el modelo en producci√≥n.

**¬øC√≥mo?**
1. Containerizar (Docker)
2. Crear API (FastAPI/Flask)
3. Implementar monitoreo
4. Establecer pipeline de actualizaci√≥n

**Herramientas:**
```python
from fastapi import FastAPI
import uvicorn

app = FastAPI()

@app.post("/predict")
async def predict(text: str):
    processed_text = limpiar_texto(text)
    vector = vectorizer.transform([processed_text])
    prediction = modelo.predict(vector)
    return {"prediccion": prediction.tolist()}
```

## üìö Fuentes Recomendadas

1. Libros:
   - "Natural Language Processing with Transformers" - Lewis Tunstall
   - "Speech and Language Processing" - Daniel Jurafsky
   - "Natural Language Processing with Python" - Steven Bird

2. Recursos en l√≠nea:
   - [Hugging Face Documentation](https://huggingface.co/docs)
   - [spaCy Course](https://course.spacy.io)
   - [Papers with Code - NLP](https://paperswithcode.com/area/natural-language-processing)
   - [Medium - Towards Data Science](https://towardsdatascience.com/)

## üîç Notas Finales

- Mant√©n un registro detallado de experimentos
- Implementa control de versiones desde el inicio
- Considera aspectos √©ticos y sesgos en los datos
- Actualiza regularmente las dependencias
- Mant√©n documentaci√≥n clara y actualizada

Recuerda: El √©xito en NLP no solo depende de la implementaci√≥n t√©cnica, sino tambi√©n de la comprensi√≥n profunda del problema y los datos. ¬°Experimenta, itera y aprende de cada proyecto!
